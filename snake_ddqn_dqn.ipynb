{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b5cafc0c-e445-414f-90a9-c083dda88c1c",
   "metadata": {},
   "source": [
    "# Double DQN - Snake game environment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69a3ac9e-2423-4856-b14f-4546b91f25f1",
   "metadata": {},
   "source": [
    "## Step 1 - Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e91e936e-98c3-48e7-810c-a275849e6e9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import gym_snakegame\n",
    "import gymnasium as gym\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import deque\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0ff8646-4c5f-47c1-b5da-70d79ffda086",
   "metadata": {},
   "source": [
    "# Step 2: Replay Buffer & Neural Network\n",
    "\n",
    "Replay Buffer: Stores experiences $(S, A, R, S', Done)$ and samples them randomly to break correlations between consecutive frames.\n",
    "\n",
    "DQN: Maps the current state (100 inputs for every cell in input space) to Q-values for every possible action (4 outputs)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "25ee46cd-c974-4b72-948f-fdf1e5b8089b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity):\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "\n",
    "    def push(self, state, action, reward, next_state, done):\n",
    "        self.buffer.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        batch = random.sample(self.buffer, batch_size)\n",
    "        states, actions, rewards, next_states, dones = map(np.array, zip(*batch))\n",
    "        return (torch.tensor(states, dtype=torch.float32).to(DEVICE),\n",
    "                torch.tensor(actions, dtype=torch.int64).to(DEVICE),\n",
    "                torch.tensor(rewards, dtype=torch.float32).to(DEVICE),\n",
    "                torch.tensor(next_states, dtype=torch.float32).to(DEVICE),\n",
    "                torch.tensor(dones, dtype=torch.float32).to(DEVICE))\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        super(DQN, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(state_dim, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, action_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "def select_action(model, state, epsilon, act_dim):\n",
    "    # Exploration: random action\n",
    "    if random.random() < epsilon:\n",
    "        return random.randrange(act_dim)\n",
    "    # Exploitation: best action according to the network\n",
    "    with torch.no_grad():\n",
    "        # Inference: NN estimates action-value function\n",
    "        state = torch.tensor(state, dtype=torch.float32).unsqueeze(0).to(DEVICE)\n",
    "        return int(torch.argmax(model(state)).item())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2978e48f-6d93-4a5f-a7b7-fc8939dcde2f",
   "metadata": {},
   "source": [
    "# Step 3: The Dual-Mode Training Function\n",
    "This function contains the core logic. It accepts a double_dqn boolean flag.\n",
    "\n",
    "Standard DQN: The Target Net finds the max Q-value directly. It is optimistic.\n",
    "\n",
    "Double DQN: The Policy Net chooses the best action (argmax), and the Target Net calculates the value of that specific action. This separation prevents the \"optimism\" from spiraling out of control."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8e388a87-f6a4-4d38-96e6-338eab1c5244",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_snake_game_experiment(exp_name, double_dqn=True, total_episodes=400):\n",
    "    env = gym.make(\n",
    "        \"gym_snakegame/SnakeGame-v0\", board_size=10, n_channel=1, n_target=1, render_mode=None # 'human' to visualize during training\n",
    "    )\n",
    "    # Space Dim. 1x10x10\n",
    "    obs_dim = np.prod(env.observation_space.shape)\n",
    "    action_dim = env.action_space.n\n",
    "    \n",
    "    # Hyperparameters\n",
    "    lr = 10e-4\n",
    "    gamma = 0.95\n",
    "    batch_size = 256\n",
    "    target_update_freq = 200\n",
    "    buffer_capacity = 100000\n",
    "    min_buffer_size = 2000\n",
    "    epsilon_decay = 300000 # Decay based on steps, not episodes\n",
    "    \n",
    "    # Initialize Networks\n",
    "    policy_net = DQN(obs_dim, action_dim).to(DEVICE)\n",
    "    target_net = DQN(obs_dim, action_dim).to(DEVICE)\n",
    "    target_net.load_state_dict(policy_net.state_dict())\n",
    "    \n",
    "    optimizer = optim.Adam(policy_net.parameters(), lr=lr)\n",
    "    replay_buffer = ReplayBuffer(buffer_capacity)\n",
    "    \n",
    "    epsilon_start = 1.0\n",
    "    epsilon_final = 0.01\n",
    "    steps_done = 0\n",
    "    rewards_history = []\n",
    "    q_value_history = [] # To track Maximization Bias\n",
    "\n",
    "    print(f\"--- Starting: {exp_name} | Double: {double_dqn} ---\")\n",
    "    episode_max_reward = -2\n",
    "    episode_min_reward = 100\n",
    "\n",
    "    for episode in range(total_episodes):\n",
    "        state, _ = env.reset()\n",
    "        episode_reward = 0\n",
    "        episode_q_vals = []\n",
    "        loss = 0\n",
    "        \n",
    "        while True:\n",
    "            # Epsilon Decay (Exponential)\n",
    "            epsilon = epsilon_final + (epsilon_start - epsilon_final) * np.exp(-1. * steps_done / epsilon_decay)\n",
    "            \n",
    "            action = select_action(policy_net, state, epsilon, action_dim)\n",
    "            next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "            done = terminated or truncated\n",
    "            \n",
    "            replay_buffer.push(state, action, reward, next_state, done)\n",
    "            state = next_state\n",
    "            episode_reward += reward\n",
    "            steps_done += 1\n",
    "\n",
    "            if len(replay_buffer) >= min_buffer_size:\n",
    "                states, actions, rewards, next_states, dones = replay_buffer.sample(batch_size)\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    if double_dqn:\n",
    "                        # --- DOUBLE DQN LOGIC ---\n",
    "                        # 1. Action Selection: Policy Net decides \"which\" action is best\n",
    "                        # Takes higher value from output layer of actions\n",
    "                        # (batch_size, output_values) => returns best action for each element in batch\n",
    "                        best_actions = policy_net(next_states).argmax(1)\n",
    "                        # 2. Action Evaluation: Target Net calculates the value of THAT action\n",
    "                        next_q = target_net(next_states).gather(1, best_actions.unsqueeze(1)).squeeze(1)\n",
    "                    else:\n",
    "                        # --- STANDARD DQN LOGIC ---\n",
    "                        # Target Net selects AND evaluates (Max operator). This causes bias.\n",
    "                        # output è una tupla: ([values], [indices]) => prende i valori\n",
    "                        next_q = target_net(next_states).max(1)[0]\n",
    "                    \n",
    "                    target = rewards + (1 - dones) * gamma * next_q\n",
    "\n",
    "                # Stime Q(s,a) per ogni azione [batch, num_actios]\n",
    "                q_values = policy_net(states)\n",
    "                # seleziona il valore Q corrispondente all’azione eseguita per ogni stato del batch\n",
    "                # array dim. [batch, 1]\n",
    "                current = q_values.gather(1, actions.unsqueeze(1)).squeeze(1)\n",
    "                \n",
    "                # Save average Q-value for analysis\n",
    "                episode_q_vals.append(current.mean().item())\n",
    "\n",
    "                loss = nn.SmoothL1Loss()(current, target)\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                torch.nn.utils.clip_grad_value_(policy_net.parameters(), 1.0)\n",
    "                optimizer.step()\n",
    "\n",
    "            if steps_done % target_update_freq == 0:\n",
    "                target_net.load_state_dict(policy_net.state_dict())\n",
    "\n",
    "            if done:\n",
    "                break\n",
    "        \n",
    "        rewards_history.append(episode_reward)\n",
    "        q_value_history.append(np.mean(episode_q_vals) if episode_q_vals else 0)\n",
    "        if episode_reward > episode_max_reward:\n",
    "            episode_max_reward = episode_reward\n",
    "        elif episode_reward < episode_min_reward:\n",
    "            episode_min_reward = episode_reward\n",
    "\n",
    "        if episode % 50 == 0:\n",
    "            print(f\"Ep {episode}: Reward {episode_reward:.2f}, Min: {episode_min_reward:.2f}, Max: {episode_max_reward:.2f} | Avg Q: {q_value_history[-1]:.2f} | Eps: {epsilon:.2f} | Loss: {loss:.4f}\")\n",
    "            episode_max_reward = -2\n",
    "            episode_min_reward = 100\n",
    "\n",
    "    env.close()\n",
    "    return rewards_history, q_value_history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b057807f-5104-4215-bcb0-8a538ad8aba0",
   "metadata": {},
   "source": [
    "# Step 4: Running the Comparison\n",
    "\n",
    "We run the training loop twice to collect data for comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b9159f31-ecd9-4581-b72e-095fe221b076",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Collecting data for Double DQN...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kevin/anaconda3/envs/rl/lib/python3.11/site-packages/pygame/pkgdata.py:25: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  from pkg_resources import resource_stream, resource_exists\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting: Double DQN | Double: True ---\n",
      "Ep 0: Reward -1.00, Min: 100.00, Max: -1.00 | Avg Q: 0.00 | Eps: 1.00 | Loss: 0.0000\n",
      "Ep 50: Reward -1.00, Min: -1.00, Max: 1.00 | Avg Q: 0.00 | Eps: 1.00 | Loss: 0.0000\n",
      "Ep 100: Reward -1.00, Min: -1.00, Max: 0.00 | Avg Q: 0.00 | Eps: 0.99 | Loss: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kevin/anaconda3/envs/rl/lib/python3.11/site-packages/gymnasium/utils/passive_env_checker.py:158: UserWarning: \u001b[33mWARN: The obs returned by the `reset()` method is not within the observation space.\u001b[0m\n",
      "  logger.warn(f\"{pre} is not within the observation space.\")\n",
      "/home/kevin/anaconda3/envs/rl/lib/python3.11/site-packages/gymnasium/utils/passive_env_checker.py:158: UserWarning: \u001b[33mWARN: The obs returned by the `step()` method is not within the observation space.\u001b[0m\n",
      "  logger.warn(f\"{pre} is not within the observation space.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 150: Reward -1.00, Min: -1.00, Max: 0.00 | Avg Q: -0.03 | Eps: 0.99 | Loss: 0.0240\n",
      "Ep 200: Reward -1.00, Min: -1.00, Max: 1.00 | Avg Q: 0.07 | Eps: 0.99 | Loss: 0.0102\n",
      "Ep 250: Reward -1.00, Min: -1.00, Max: 1.00 | Avg Q: 0.21 | Eps: 0.99 | Loss: 0.0032\n",
      "Ep 300: Reward -1.00, Min: -1.00, Max: 0.00 | Avg Q: 0.30 | Eps: 0.98 | Loss: 0.0092\n",
      "Ep 350: Reward -1.00, Min: -1.00, Max: 0.00 | Avg Q: 0.36 | Eps: 0.98 | Loss: 0.0021\n",
      "Ep 400: Reward 0.00, Min: -1.00, Max: 0.00 | Avg Q: 0.43 | Eps: 0.98 | Loss: 0.0036\n",
      "Ep 450: Reward -1.00, Min: -1.00, Max: 1.00 | Avg Q: 0.50 | Eps: 0.98 | Loss: 0.0028\n",
      "Ep 500: Reward -1.00, Min: -1.00, Max: 0.00 | Avg Q: 0.53 | Eps: 0.97 | Loss: 0.0028\n",
      "Ep 550: Reward -1.00, Min: -1.00, Max: 0.00 | Avg Q: 0.60 | Eps: 0.97 | Loss: 0.0030\n",
      "Ep 600: Reward -1.00, Min: -1.00, Max: 0.00 | Avg Q: 0.67 | Eps: 0.97 | Loss: 0.0065\n",
      "Ep 650: Reward -1.00, Min: -1.00, Max: 1.00 | Avg Q: 0.75 | Eps: 0.97 | Loss: 0.0027\n",
      "Ep 700: Reward -1.00, Min: -1.00, Max: 1.00 | Avg Q: 0.78 | Eps: 0.96 | Loss: 0.0137\n",
      "Ep 750: Reward -1.00, Min: -1.00, Max: 1.00 | Avg Q: 0.83 | Eps: 0.96 | Loss: 0.0025\n",
      "Ep 800: Reward -1.00, Min: -1.00, Max: 0.00 | Avg Q: 0.83 | Eps: 0.96 | Loss: 0.0098\n",
      "Ep 850: Reward -1.00, Min: -1.00, Max: 0.00 | Avg Q: 0.89 | Eps: 0.96 | Loss: 0.0024\n",
      "Ep 900: Reward 0.00, Min: -1.00, Max: 0.00 | Avg Q: 0.90 | Eps: 0.95 | Loss: 0.0062\n",
      "Ep 950: Reward -1.00, Min: -1.00, Max: 1.00 | Avg Q: 0.92 | Eps: 0.95 | Loss: 0.0085\n",
      "Ep 1000: Reward -1.00, Min: -1.00, Max: 1.00 | Avg Q: 0.94 | Eps: 0.95 | Loss: 0.0055\n",
      "Ep 1050: Reward -1.00, Min: -1.00, Max: 1.00 | Avg Q: 0.94 | Eps: 0.95 | Loss: 0.0065\n",
      "Ep 1100: Reward -1.00, Min: -1.00, Max: 1.00 | Avg Q: 0.96 | Eps: 0.95 | Loss: 0.0027\n",
      "Ep 1150: Reward -1.00, Min: -1.00, Max: 1.00 | Avg Q: 0.96 | Eps: 0.94 | Loss: 0.0038\n",
      "Ep 1200: Reward -1.00, Min: -1.00, Max: 1.00 | Avg Q: 1.00 | Eps: 0.94 | Loss: 0.0070\n",
      "Ep 1250: Reward -1.00, Min: -1.00, Max: 1.00 | Avg Q: 1.02 | Eps: 0.94 | Loss: 0.0055\n",
      "Ep 1300: Reward -1.00, Min: -1.00, Max: 1.00 | Avg Q: 1.01 | Eps: 0.94 | Loss: 0.0113\n",
      "Ep 1350: Reward -1.00, Min: -1.00, Max: 2.00 | Avg Q: 1.04 | Eps: 0.93 | Loss: 0.0025\n",
      "Ep 1400: Reward -1.00, Min: -1.00, Max: 1.00 | Avg Q: 1.06 | Eps: 0.93 | Loss: 0.0123\n",
      "Ep 1450: Reward -1.00, Min: -1.00, Max: 0.00 | Avg Q: 1.09 | Eps: 0.93 | Loss: 0.0072\n",
      "Ep 1500: Reward -1.00, Min: -1.00, Max: 1.00 | Avg Q: 1.11 | Eps: 0.93 | Loss: 0.0057\n",
      "Ep 1550: Reward -1.00, Min: -1.00, Max: 0.00 | Avg Q: 1.13 | Eps: 0.92 | Loss: 0.0021\n",
      "Ep 1600: Reward -1.00, Min: -1.00, Max: 0.00 | Avg Q: 1.18 | Eps: 0.92 | Loss: 0.0027\n",
      "Ep 1650: Reward -1.00, Min: -1.00, Max: 0.00 | Avg Q: 1.21 | Eps: 0.92 | Loss: 0.0155\n",
      "Ep 1700: Reward -1.00, Min: -1.00, Max: 1.00 | Avg Q: 1.19 | Eps: 0.92 | Loss: 0.0056\n",
      "Ep 1750: Reward 0.00, Min: -1.00, Max: 1.00 | Avg Q: 1.25 | Eps: 0.91 | Loss: 0.0034\n",
      "Ep 1800: Reward 0.00, Min: -1.00, Max: 0.00 | Avg Q: 1.30 | Eps: 0.91 | Loss: 0.0092\n",
      "Ep 1850: Reward -1.00, Min: -1.00, Max: 1.00 | Avg Q: 1.34 | Eps: 0.91 | Loss: 0.0021\n",
      "Ep 1900: Reward -1.00, Min: -1.00, Max: 0.00 | Avg Q: 1.35 | Eps: 0.91 | Loss: 0.0042\n",
      "Ep 1950: Reward -1.00, Min: -1.00, Max: 1.00 | Avg Q: 1.34 | Eps: 0.90 | Loss: 0.0103\n",
      "Ep 2000: Reward -1.00, Min: -1.00, Max: 1.00 | Avg Q: 1.36 | Eps: 0.90 | Loss: 0.0040\n",
      "Ep 2050: Reward 0.00, Min: -1.00, Max: 1.00 | Avg Q: 1.37 | Eps: 0.90 | Loss: 0.0091\n",
      "Ep 2100: Reward -1.00, Min: -1.00, Max: 0.00 | Avg Q: 1.37 | Eps: 0.90 | Loss: 0.0163\n",
      "Ep 2150: Reward 0.00, Min: -1.00, Max: 0.00 | Avg Q: 1.44 | Eps: 0.89 | Loss: 0.0044\n",
      "Ep 2200: Reward 0.00, Min: -1.00, Max: 0.00 | Avg Q: 1.42 | Eps: 0.89 | Loss: 0.0054\n",
      "Ep 2250: Reward -1.00, Min: -1.00, Max: 1.00 | Avg Q: 1.51 | Eps: 0.89 | Loss: 0.0060\n",
      "Ep 2300: Reward 0.00, Min: -1.00, Max: 0.00 | Avg Q: 1.54 | Eps: 0.89 | Loss: 0.0041\n",
      "Ep 2350: Reward -1.00, Min: -1.00, Max: 1.00 | Avg Q: 1.56 | Eps: 0.88 | Loss: 0.0065\n",
      "Ep 2400: Reward -1.00, Min: -1.00, Max: 0.00 | Avg Q: 1.61 | Eps: 0.88 | Loss: 0.0074\n",
      "Ep 2450: Reward -1.00, Min: -1.00, Max: 2.00 | Avg Q: 1.63 | Eps: 0.88 | Loss: 0.0140\n",
      "Ep 2500: Reward 0.00, Min: -1.00, Max: 1.00 | Avg Q: 1.65 | Eps: 0.88 | Loss: 0.0071\n",
      "Ep 2550: Reward -1.00, Min: -1.00, Max: 0.00 | Avg Q: 1.62 | Eps: 0.87 | Loss: 0.0058\n",
      "Ep 2600: Reward -1.00, Min: -1.00, Max: 1.00 | Avg Q: 1.62 | Eps: 0.87 | Loss: 0.0027\n",
      "Ep 2650: Reward -1.00, Min: -1.00, Max: 1.00 | Avg Q: 1.63 | Eps: 0.87 | Loss: 0.0060\n",
      "Ep 2700: Reward -1.00, Min: -1.00, Max: 1.00 | Avg Q: 1.67 | Eps: 0.87 | Loss: 0.0043\n",
      "Ep 2750: Reward 0.00, Min: -1.00, Max: 1.00 | Avg Q: 1.67 | Eps: 0.86 | Loss: 0.0041\n",
      "Ep 2800: Reward -1.00, Min: -1.00, Max: 1.00 | Avg Q: 1.68 | Eps: 0.86 | Loss: 0.0089\n",
      "Ep 2850: Reward 1.00, Min: -1.00, Max: 1.00 | Avg Q: 1.70 | Eps: 0.86 | Loss: 0.0039\n",
      "Ep 2900: Reward -1.00, Min: -1.00, Max: 0.00 | Avg Q: 1.71 | Eps: 0.86 | Loss: 0.0039\n",
      "Ep 2950: Reward 0.00, Min: -1.00, Max: 1.00 | Avg Q: 1.71 | Eps: 0.85 | Loss: 0.0037\n",
      "Ep 3000: Reward -1.00, Min: -1.00, Max: 2.00 | Avg Q: 1.73 | Eps: 0.85 | Loss: 0.0075\n",
      "Ep 3050: Reward -1.00, Min: -1.00, Max: 2.00 | Avg Q: 1.71 | Eps: 0.85 | Loss: 0.0042\n",
      "Ep 3100: Reward -1.00, Min: -1.00, Max: 2.00 | Avg Q: 1.75 | Eps: 0.85 | Loss: 0.0090\n",
      "Ep 3150: Reward 0.00, Min: -1.00, Max: 0.00 | Avg Q: 1.79 | Eps: 0.84 | Loss: 0.0097\n",
      "Ep 3200: Reward -1.00, Min: -1.00, Max: 2.00 | Avg Q: 1.82 | Eps: 0.84 | Loss: 0.0062\n",
      "Ep 3250: Reward -1.00, Min: -1.00, Max: 2.00 | Avg Q: 1.87 | Eps: 0.84 | Loss: 0.0069\n",
      "Ep 3300: Reward -1.00, Min: -1.00, Max: 1.00 | Avg Q: 1.87 | Eps: 0.84 | Loss: 0.0052\n",
      "Ep 3350: Reward -1.00, Min: -1.00, Max: 1.00 | Avg Q: 1.88 | Eps: 0.83 | Loss: 0.0036\n",
      "Ep 3400: Reward -1.00, Min: -1.00, Max: 1.00 | Avg Q: 1.93 | Eps: 0.83 | Loss: 0.0035\n",
      "Ep 3450: Reward -1.00, Min: -1.00, Max: 1.00 | Avg Q: 1.92 | Eps: 0.83 | Loss: 0.0155\n",
      "Ep 3500: Reward -1.00, Min: -1.00, Max: 1.00 | Avg Q: 1.98 | Eps: 0.83 | Loss: 0.0062\n",
      "Ep 3550: Reward -1.00, Min: -1.00, Max: 1.00 | Avg Q: 2.00 | Eps: 0.82 | Loss: 0.0145\n",
      "Ep 3600: Reward -1.00, Min: -1.00, Max: 1.00 | Avg Q: 2.02 | Eps: 0.82 | Loss: 0.0023\n",
      "Ep 3650: Reward 2.00, Min: -1.00, Max: 2.00 | Avg Q: 2.08 | Eps: 0.82 | Loss: 0.0080\n",
      "Ep 3700: Reward 0.00, Min: -1.00, Max: 2.00 | Avg Q: 2.13 | Eps: 0.82 | Loss: 0.0095\n",
      "Ep 3750: Reward 0.00, Min: -1.00, Max: 1.00 | Avg Q: 2.14 | Eps: 0.81 | Loss: 0.0131\n",
      "Ep 3800: Reward -1.00, Min: -1.00, Max: 2.00 | Avg Q: 2.16 | Eps: 0.81 | Loss: 0.0066\n",
      "Ep 3850: Reward 0.00, Min: -1.00, Max: 3.00 | Avg Q: 2.19 | Eps: 0.81 | Loss: 0.0053\n",
      "Ep 3900: Reward 0.00, Min: -1.00, Max: 4.00 | Avg Q: 2.22 | Eps: 0.81 | Loss: 0.0045\n",
      "Ep 3950: Reward 0.00, Min: -1.00, Max: 2.00 | Avg Q: 2.24 | Eps: 0.80 | Loss: 0.0216\n",
      "Ep 4000: Reward -1.00, Min: -1.00, Max: 1.00 | Avg Q: 2.22 | Eps: 0.80 | Loss: 0.0190\n",
      "Ep 4050: Reward 2.00, Min: -1.00, Max: 3.00 | Avg Q: 2.22 | Eps: 0.80 | Loss: 0.0102\n",
      "Ep 4100: Reward 0.00, Min: -1.00, Max: 2.00 | Avg Q: 2.22 | Eps: 0.80 | Loss: 0.0063\n",
      "Ep 4150: Reward -1.00, Min: -1.00, Max: 2.00 | Avg Q: 2.30 | Eps: 0.79 | Loss: 0.0076\n",
      "Ep 4200: Reward -1.00, Min: -1.00, Max: 1.00 | Avg Q: 2.31 | Eps: 0.79 | Loss: 0.0236\n",
      "Ep 4250: Reward 0.00, Min: -1.00, Max: 1.00 | Avg Q: 2.31 | Eps: 0.79 | Loss: 0.0037\n",
      "Ep 4300: Reward -1.00, Min: -1.00, Max: 2.00 | Avg Q: 2.29 | Eps: 0.79 | Loss: 0.0075\n",
      "Ep 4350: Reward 1.00, Min: -1.00, Max: 1.00 | Avg Q: 2.30 | Eps: 0.78 | Loss: 0.0171\n",
      "Ep 4400: Reward -1.00, Min: -1.00, Max: 2.00 | Avg Q: 2.33 | Eps: 0.78 | Loss: 0.0184\n",
      "Ep 4450: Reward 0.00, Min: -1.00, Max: 2.00 | Avg Q: 2.35 | Eps: 0.78 | Loss: 0.0062\n",
      "Ep 4500: Reward 0.00, Min: -1.00, Max: 1.00 | Avg Q: 2.35 | Eps: 0.78 | Loss: 0.0046\n",
      "Ep 4550: Reward 1.00, Min: -1.00, Max: 2.00 | Avg Q: 2.34 | Eps: 0.77 | Loss: 0.0047\n",
      "Ep 4600: Reward -1.00, Min: -1.00, Max: 2.00 | Avg Q: 2.35 | Eps: 0.77 | Loss: 0.0196\n",
      "Ep 4650: Reward 1.00, Min: -1.00, Max: 1.00 | Avg Q: 2.34 | Eps: 0.77 | Loss: 0.0036\n",
      "Ep 4700: Reward -1.00, Min: -1.00, Max: 2.00 | Avg Q: 2.36 | Eps: 0.77 | Loss: 0.0137\n",
      "Ep 4750: Reward -1.00, Min: -1.00, Max: 2.00 | Avg Q: 2.31 | Eps: 0.76 | Loss: 0.0102\n",
      "Ep 4800: Reward -1.00, Min: -1.00, Max: 1.00 | Avg Q: 2.33 | Eps: 0.76 | Loss: 0.0104\n",
      "Ep 4850: Reward -1.00, Min: -1.00, Max: 1.00 | Avg Q: 2.36 | Eps: 0.76 | Loss: 0.0234\n",
      "Ep 4900: Reward 0.00, Min: -1.00, Max: 2.00 | Avg Q: 2.36 | Eps: 0.76 | Loss: 0.0062\n",
      "Ep 4950: Reward -1.00, Min: -1.00, Max: 3.00 | Avg Q: 2.34 | Eps: 0.75 | Loss: 0.0083\n",
      "Ep 5000: Reward 0.00, Min: -1.00, Max: 4.00 | Avg Q: 2.31 | Eps: 0.75 | Loss: 0.0112\n",
      "Ep 5050: Reward 1.00, Min: -1.00, Max: 3.00 | Avg Q: 2.37 | Eps: 0.75 | Loss: 0.0031\n",
      "Ep 5100: Reward -1.00, Min: -1.00, Max: 1.00 | Avg Q: 2.36 | Eps: 0.75 | Loss: 0.0101\n",
      "Ep 5150: Reward -1.00, Min: -1.00, Max: 4.00 | Avg Q: 2.40 | Eps: 0.74 | Loss: 0.0115\n",
      "Ep 5200: Reward -1.00, Min: -1.00, Max: 3.00 | Avg Q: 2.43 | Eps: 0.74 | Loss: 0.0082\n",
      "Ep 5250: Reward -1.00, Min: -1.00, Max: 2.00 | Avg Q: 2.41 | Eps: 0.74 | Loss: 0.0038\n",
      "Ep 5300: Reward 0.00, Min: -1.00, Max: 2.00 | Avg Q: 2.43 | Eps: 0.74 | Loss: 0.0130\n",
      "Ep 5350: Reward -1.00, Min: -1.00, Max: 1.00 | Avg Q: 2.46 | Eps: 0.74 | Loss: 0.0268\n",
      "Ep 5400: Reward -1.00, Min: -1.00, Max: 1.00 | Avg Q: 2.49 | Eps: 0.73 | Loss: 0.0094\n",
      "Ep 5450: Reward 1.00, Min: -1.00, Max: 2.00 | Avg Q: 2.42 | Eps: 0.73 | Loss: 0.0188\n",
      "Ep 5500: Reward 0.00, Min: -1.00, Max: 1.00 | Avg Q: 2.41 | Eps: 0.73 | Loss: 0.0049\n",
      "Ep 5550: Reward 0.00, Min: -1.00, Max: 2.00 | Avg Q: 2.44 | Eps: 0.73 | Loss: 0.0250\n",
      "Ep 5600: Reward -1.00, Min: -1.00, Max: 3.00 | Avg Q: 2.44 | Eps: 0.72 | Loss: 0.0280\n",
      "Ep 5650: Reward -1.00, Min: -1.00, Max: 2.00 | Avg Q: 2.47 | Eps: 0.72 | Loss: 0.0034\n",
      "Ep 5700: Reward 0.00, Min: -1.00, Max: 2.00 | Avg Q: 2.48 | Eps: 0.72 | Loss: 0.0110\n",
      "Ep 5750: Reward -1.00, Min: -1.00, Max: 3.00 | Avg Q: 2.43 | Eps: 0.72 | Loss: 0.0049\n",
      "Ep 5800: Reward -1.00, Min: -1.00, Max: 2.00 | Avg Q: 2.41 | Eps: 0.72 | Loss: 0.0133\n",
      "Ep 5850: Reward -1.00, Min: -1.00, Max: 3.00 | Avg Q: 2.40 | Eps: 0.71 | Loss: 0.0177\n",
      "Ep 5900: Reward 0.00, Min: -1.00, Max: 2.00 | Avg Q: 2.42 | Eps: 0.71 | Loss: 0.0159\n",
      "Ep 5950: Reward -1.00, Min: -1.00, Max: 1.00 | Avg Q: 2.42 | Eps: 0.71 | Loss: 0.0087\n",
      "Ep 6000: Reward 1.00, Min: -1.00, Max: 5.00 | Avg Q: 2.42 | Eps: 0.71 | Loss: 0.0047\n",
      "Ep 6050: Reward 0.00, Min: -1.00, Max: 4.00 | Avg Q: 2.46 | Eps: 0.70 | Loss: 0.0056\n",
      "Ep 6100: Reward 0.00, Min: -1.00, Max: 3.00 | Avg Q: 2.45 | Eps: 0.70 | Loss: 0.0100\n",
      "Ep 6150: Reward -1.00, Min: -1.00, Max: 3.00 | Avg Q: 2.43 | Eps: 0.70 | Loss: 0.0055\n",
      "Ep 6200: Reward 0.00, Min: -1.00, Max: 3.00 | Avg Q: 2.46 | Eps: 0.70 | Loss: 0.0107\n",
      "Ep 6250: Reward -1.00, Min: -1.00, Max: 1.00 | Avg Q: 2.44 | Eps: 0.69 | Loss: 0.0139\n",
      "Ep 6300: Reward 0.00, Min: -1.00, Max: 3.00 | Avg Q: 2.45 | Eps: 0.69 | Loss: 0.0232\n",
      "Ep 6350: Reward 2.00, Min: -1.00, Max: 4.00 | Avg Q: 2.45 | Eps: 0.69 | Loss: 0.0219\n",
      "Ep 6400: Reward 0.00, Min: -1.00, Max: 2.00 | Avg Q: 2.47 | Eps: 0.69 | Loss: 0.0287\n",
      "Ep 6450: Reward -1.00, Min: -1.00, Max: 3.00 | Avg Q: 2.48 | Eps: 0.69 | Loss: 0.0397\n",
      "Ep 6500: Reward 2.00, Min: -1.00, Max: 2.00 | Avg Q: 2.52 | Eps: 0.68 | Loss: 0.0053\n",
      "Ep 6550: Reward 0.00, Min: -1.00, Max: 3.00 | Avg Q: 2.52 | Eps: 0.68 | Loss: 0.0193\n",
      "Ep 6600: Reward 0.00, Min: -1.00, Max: 2.00 | Avg Q: 2.54 | Eps: 0.68 | Loss: 0.0097\n",
      "Ep 6650: Reward 0.00, Min: -1.00, Max: 4.00 | Avg Q: 2.51 | Eps: 0.68 | Loss: 0.0257\n",
      "Ep 6700: Reward 0.00, Min: -1.00, Max: 2.00 | Avg Q: 2.49 | Eps: 0.67 | Loss: 0.0048\n",
      "Ep 6750: Reward 0.00, Min: -1.00, Max: 2.00 | Avg Q: 2.46 | Eps: 0.67 | Loss: 0.0169\n",
      "Ep 6800: Reward -1.00, Min: -1.00, Max: 4.00 | Avg Q: 2.48 | Eps: 0.67 | Loss: 0.0065\n",
      "Ep 6850: Reward 0.00, Min: -1.00, Max: 3.00 | Avg Q: 2.47 | Eps: 0.67 | Loss: 0.0188\n",
      "Ep 6900: Reward -1.00, Min: -1.00, Max: 2.00 | Avg Q: 2.50 | Eps: 0.66 | Loss: 0.0055\n",
      "Ep 6950: Reward 0.00, Min: -1.00, Max: 3.00 | Avg Q: 2.47 | Eps: 0.66 | Loss: 0.0194\n",
      "Ep 7000: Reward 1.00, Min: -1.00, Max: 3.00 | Avg Q: 2.54 | Eps: 0.66 | Loss: 0.0132\n",
      "Ep 7050: Reward -1.00, Min: -1.00, Max: 4.00 | Avg Q: 2.54 | Eps: 0.66 | Loss: 0.0040\n",
      "Ep 7100: Reward 0.00, Min: -1.00, Max: 2.00 | Avg Q: 2.54 | Eps: 0.66 | Loss: 0.0225\n",
      "Ep 7150: Reward 0.00, Min: -1.00, Max: 5.00 | Avg Q: 2.54 | Eps: 0.65 | Loss: 0.0081\n",
      "Ep 7200: Reward 1.00, Min: -1.00, Max: 3.00 | Avg Q: 2.53 | Eps: 0.65 | Loss: 0.0391\n",
      "Ep 7250: Reward 0.00, Min: -1.00, Max: 3.00 | Avg Q: 2.56 | Eps: 0.65 | Loss: 0.0277\n",
      "Ep 7300: Reward -1.00, Min: -1.00, Max: 6.00 | Avg Q: 2.54 | Eps: 0.65 | Loss: 0.0035\n",
      "Ep 7350: Reward 0.00, Min: -1.00, Max: 2.00 | Avg Q: 2.54 | Eps: 0.65 | Loss: 0.0368\n",
      "Ep 7400: Reward -1.00, Min: -1.00, Max: 4.00 | Avg Q: 2.56 | Eps: 0.64 | Loss: 0.0179\n",
      "Ep 7450: Reward -1.00, Min: -1.00, Max: 2.00 | Avg Q: 2.54 | Eps: 0.64 | Loss: 0.0050\n",
      "Ep 7500: Reward 0.00, Min: -1.00, Max: 4.00 | Avg Q: 2.56 | Eps: 0.64 | Loss: 0.0273\n",
      "Ep 7550: Reward 2.00, Min: -1.00, Max: 2.00 | Avg Q: 2.62 | Eps: 0.64 | Loss: 0.0213\n",
      "Ep 7600: Reward 0.00, Min: -1.00, Max: 3.00 | Avg Q: 2.59 | Eps: 0.63 | Loss: 0.0254\n",
      "Ep 7650: Reward 0.00, Min: -1.00, Max: 4.00 | Avg Q: 2.58 | Eps: 0.63 | Loss: 0.0146\n",
      "Ep 7700: Reward 2.00, Min: -1.00, Max: 5.00 | Avg Q: 2.59 | Eps: 0.63 | Loss: 0.0107\n",
      "Ep 7750: Reward 0.00, Min: -1.00, Max: 2.00 | Avg Q: 2.56 | Eps: 0.63 | Loss: 0.0362\n",
      "Ep 7800: Reward 1.00, Min: -1.00, Max: 3.00 | Avg Q: 2.56 | Eps: 0.63 | Loss: 0.0172\n",
      "Ep 7850: Reward 0.00, Min: -1.00, Max: 4.00 | Avg Q: 2.49 | Eps: 0.62 | Loss: 0.0319\n",
      "Ep 7900: Reward 0.00, Min: -1.00, Max: 5.00 | Avg Q: 2.50 | Eps: 0.62 | Loss: 0.0218\n",
      "Ep 7950: Reward -1.00, Min: -1.00, Max: 2.00 | Avg Q: 2.50 | Eps: 0.62 | Loss: 0.0362\n",
      "Ep 8000: Reward 0.00, Min: -1.00, Max: 3.00 | Avg Q: 2.50 | Eps: 0.62 | Loss: 0.0217\n",
      "Ep 8050: Reward 2.00, Min: -1.00, Max: 3.00 | Avg Q: 2.52 | Eps: 0.62 | Loss: 0.0133\n",
      "Ep 8100: Reward 1.00, Min: -1.00, Max: 3.00 | Avg Q: 2.53 | Eps: 0.61 | Loss: 0.0323\n",
      "Ep 8150: Reward 1.00, Min: -1.00, Max: 5.00 | Avg Q: 2.54 | Eps: 0.61 | Loss: 0.0069\n",
      "Ep 8200: Reward 2.00, Min: -1.00, Max: 3.00 | Avg Q: 2.53 | Eps: 0.61 | Loss: 0.0326\n",
      "Ep 8250: Reward 0.00, Min: -1.00, Max: 3.00 | Avg Q: 2.52 | Eps: 0.61 | Loss: 0.0042\n",
      "Ep 8300: Reward -1.00, Min: -1.00, Max: 3.00 | Avg Q: 2.54 | Eps: 0.61 | Loss: 0.0297\n",
      "Ep 8350: Reward 0.00, Min: -1.00, Max: 3.00 | Avg Q: 2.56 | Eps: 0.60 | Loss: 0.0090\n",
      "Ep 8400: Reward 0.00, Min: -1.00, Max: 9.00 | Avg Q: 2.57 | Eps: 0.60 | Loss: 0.0128\n",
      "Ep 8450: Reward -1.00, Min: -1.00, Max: 3.00 | Avg Q: 2.56 | Eps: 0.60 | Loss: 0.0407\n",
      "Ep 8500: Reward 2.00, Min: -1.00, Max: 4.00 | Avg Q: 2.55 | Eps: 0.60 | Loss: 0.0311\n",
      "Ep 8550: Reward -1.00, Min: -1.00, Max: 4.00 | Avg Q: 2.52 | Eps: 0.59 | Loss: 0.0316\n",
      "Ep 8600: Reward -1.00, Min: -1.00, Max: 3.00 | Avg Q: 2.48 | Eps: 0.59 | Loss: 0.0296\n",
      "Ep 8650: Reward 1.00, Min: -1.00, Max: 4.00 | Avg Q: 2.55 | Eps: 0.59 | Loss: 0.0214\n",
      "Ep 8700: Reward 0.00, Min: -1.00, Max: 4.00 | Avg Q: 2.58 | Eps: 0.59 | Loss: 0.0105\n",
      "Ep 8750: Reward 1.00, Min: -1.00, Max: 2.00 | Avg Q: 2.58 | Eps: 0.59 | Loss: 0.0084\n",
      "Ep 8800: Reward 1.00, Min: -1.00, Max: 6.00 | Avg Q: 2.55 | Eps: 0.58 | Loss: 0.0179\n",
      "Ep 8850: Reward -1.00, Min: -1.00, Max: 3.00 | Avg Q: 2.52 | Eps: 0.58 | Loss: 0.0067\n",
      "Ep 8900: Reward 1.00, Min: -1.00, Max: 3.00 | Avg Q: 2.54 | Eps: 0.58 | Loss: 0.0068\n",
      "Ep 8950: Reward 0.00, Min: -1.00, Max: 5.00 | Avg Q: 2.56 | Eps: 0.58 | Loss: 0.0121\n",
      "Ep 9000: Reward 0.00, Min: -1.00, Max: 3.00 | Avg Q: 2.56 | Eps: 0.57 | Loss: 0.0187\n",
      "Ep 9050: Reward 1.00, Min: -1.00, Max: 5.00 | Avg Q: 2.54 | Eps: 0.57 | Loss: 0.0225\n",
      "Ep 9100: Reward 4.00, Min: -1.00, Max: 4.00 | Avg Q: 2.52 | Eps: 0.57 | Loss: 0.0170\n",
      "Ep 9150: Reward -1.00, Min: -1.00, Max: 5.00 | Avg Q: 2.55 | Eps: 0.57 | Loss: 0.0350\n",
      "Ep 9200: Reward 1.00, Min: -1.00, Max: 3.00 | Avg Q: 2.53 | Eps: 0.57 | Loss: 0.0382\n",
      "Ep 9250: Reward 1.00, Min: -1.00, Max: 5.00 | Avg Q: 2.54 | Eps: 0.56 | Loss: 0.0195\n",
      "Ep 9300: Reward 0.00, Min: -1.00, Max: 3.00 | Avg Q: 2.52 | Eps: 0.56 | Loss: 0.0125\n",
      "Ep 9350: Reward 0.00, Min: -1.00, Max: 7.00 | Avg Q: 2.51 | Eps: 0.56 | Loss: 0.0352\n",
      "Ep 9400: Reward 0.00, Min: -1.00, Max: 4.00 | Avg Q: 2.50 | Eps: 0.56 | Loss: 0.0179\n",
      "Ep 9450: Reward 2.00, Min: -1.00, Max: 3.00 | Avg Q: 2.51 | Eps: 0.56 | Loss: 0.0168\n",
      "Ep 9500: Reward 0.00, Min: -1.00, Max: 3.00 | Avg Q: 2.52 | Eps: 0.56 | Loss: 0.0167\n",
      "Ep 9550: Reward 0.00, Min: -1.00, Max: 4.00 | Avg Q: 2.52 | Eps: 0.55 | Loss: 0.0311\n",
      "Ep 9600: Reward 0.00, Min: -1.00, Max: 3.00 | Avg Q: 2.55 | Eps: 0.55 | Loss: 0.0148\n",
      "Ep 9650: Reward 1.00, Min: -1.00, Max: 4.00 | Avg Q: 2.56 | Eps: 0.55 | Loss: 0.0359\n",
      "Ep 9700: Reward 0.00, Min: -1.00, Max: 3.00 | Avg Q: 2.60 | Eps: 0.55 | Loss: 0.0152\n",
      "Ep 9750: Reward 2.00, Min: -1.00, Max: 2.00 | Avg Q: 2.61 | Eps: 0.55 | Loss: 0.0113\n",
      "Ep 9800: Reward 0.00, Min: -1.00, Max: 3.00 | Avg Q: 2.60 | Eps: 0.54 | Loss: 0.0170\n",
      "Ep 9850: Reward 0.00, Min: -1.00, Max: 3.00 | Avg Q: 2.57 | Eps: 0.54 | Loss: 0.0300\n",
      "Ep 9900: Reward 0.00, Min: -1.00, Max: 6.00 | Avg Q: 2.58 | Eps: 0.54 | Loss: 0.0159\n",
      "Ep 9950: Reward 2.00, Min: -1.00, Max: 5.00 | Avg Q: 2.59 | Eps: 0.54 | Loss: 0.0318\n",
      "Ep 10000: Reward 3.00, Min: -1.00, Max: 4.00 | Avg Q: 2.61 | Eps: 0.54 | Loss: 0.0076\n",
      "Ep 10050: Reward -1.00, Min: -1.00, Max: 3.00 | Avg Q: 2.61 | Eps: 0.53 | Loss: 0.0274\n",
      "Ep 10100: Reward -1.00, Min: -1.00, Max: 6.00 | Avg Q: 2.62 | Eps: 0.53 | Loss: 0.0190\n",
      "Ep 10150: Reward 1.00, Min: -1.00, Max: 4.00 | Avg Q: 2.65 | Eps: 0.53 | Loss: 0.0338\n",
      "Ep 10200: Reward 1.00, Min: -1.00, Max: 4.00 | Avg Q: 2.64 | Eps: 0.53 | Loss: 0.0111\n",
      "Ep 10250: Reward 0.00, Min: -1.00, Max: 4.00 | Avg Q: 2.58 | Eps: 0.53 | Loss: 0.0149\n",
      "Ep 10300: Reward 1.00, Min: -1.00, Max: 3.00 | Avg Q: 2.63 | Eps: 0.52 | Loss: 0.0408\n",
      "Ep 10350: Reward 0.00, Min: -1.00, Max: 4.00 | Avg Q: 2.64 | Eps: 0.52 | Loss: 0.0263\n",
      "Ep 10400: Reward 0.00, Min: -1.00, Max: 4.00 | Avg Q: 2.65 | Eps: 0.52 | Loss: 0.0334\n",
      "Ep 10450: Reward 1.00, Min: -1.00, Max: 3.00 | Avg Q: 2.63 | Eps: 0.52 | Loss: 0.0158\n",
      "Ep 10500: Reward -1.00, Min: -1.00, Max: 4.00 | Avg Q: 2.65 | Eps: 0.52 | Loss: 0.0313\n",
      "Ep 10550: Reward -1.00, Min: -1.00, Max: 4.00 | Avg Q: 2.60 | Eps: 0.51 | Loss: 0.0102\n",
      "Ep 10600: Reward 0.00, Min: -1.00, Max: 4.00 | Avg Q: 2.64 | Eps: 0.51 | Loss: 0.0362\n",
      "Ep 10650: Reward 1.00, Min: -1.00, Max: 3.00 | Avg Q: 2.64 | Eps: 0.51 | Loss: 0.0252\n",
      "Ep 10700: Reward 0.00, Min: -1.00, Max: 6.00 | Avg Q: 2.61 | Eps: 0.51 | Loss: 0.0388\n",
      "Ep 10750: Reward 1.00, Min: -1.00, Max: 4.00 | Avg Q: 2.55 | Eps: 0.51 | Loss: 0.0552\n",
      "Ep 10800: Reward -1.00, Min: -1.00, Max: 5.00 | Avg Q: 2.60 | Eps: 0.50 | Loss: 0.0105\n",
      "Ep 10850: Reward 0.00, Min: -1.00, Max: 4.00 | Avg Q: 2.58 | Eps: 0.50 | Loss: 0.0070\n",
      "Ep 10900: Reward 0.00, Min: -1.00, Max: 6.00 | Avg Q: 2.58 | Eps: 0.50 | Loss: 0.0096\n",
      "Ep 10950: Reward 1.00, Min: -1.00, Max: 7.00 | Avg Q: 2.65 | Eps: 0.50 | Loss: 0.0263\n",
      "Ep 11000: Reward 0.00, Min: -1.00, Max: 4.00 | Avg Q: 2.60 | Eps: 0.50 | Loss: 0.0270\n",
      "Ep 11050: Reward -1.00, Min: -1.00, Max: 8.00 | Avg Q: 2.63 | Eps: 0.49 | Loss: 0.0326\n",
      "Ep 11100: Reward -1.00, Min: -1.00, Max: 5.00 | Avg Q: 2.63 | Eps: 0.49 | Loss: 0.0285\n",
      "Ep 11150: Reward 0.00, Min: -1.00, Max: 6.00 | Avg Q: 2.60 | Eps: 0.49 | Loss: 0.0164\n",
      "Ep 11200: Reward 1.00, Min: -1.00, Max: 5.00 | Avg Q: 2.58 | Eps: 0.49 | Loss: 0.0072\n",
      "Ep 11250: Reward -1.00, Min: -1.00, Max: 3.00 | Avg Q: 2.61 | Eps: 0.49 | Loss: 0.0124\n",
      "Ep 11300: Reward 2.00, Min: -1.00, Max: 5.00 | Avg Q: 2.58 | Eps: 0.49 | Loss: 0.0116\n",
      "Ep 11350: Reward 2.00, Min: -1.00, Max: 4.00 | Avg Q: 2.58 | Eps: 0.48 | Loss: 0.0444\n",
      "Ep 11400: Reward 0.00, Min: -1.00, Max: 6.00 | Avg Q: 2.58 | Eps: 0.48 | Loss: 0.0435\n",
      "Ep 11450: Reward 0.00, Min: -1.00, Max: 3.00 | Avg Q: 2.57 | Eps: 0.48 | Loss: 0.0317\n",
      "Ep 11500: Reward -1.00, Min: -1.00, Max: 6.00 | Avg Q: 2.56 | Eps: 0.48 | Loss: 0.0262\n",
      "Ep 11550: Reward -1.00, Min: -1.00, Max: 10.00 | Avg Q: 2.62 | Eps: 0.48 | Loss: 0.0293\n",
      "Ep 11600: Reward 0.00, Min: -1.00, Max: 8.00 | Avg Q: 2.57 | Eps: 0.47 | Loss: 0.0214\n",
      "Ep 11650: Reward -1.00, Min: -1.00, Max: 9.00 | Avg Q: 2.63 | Eps: 0.47 | Loss: 0.0171\n",
      "Ep 11700: Reward 0.00, Min: -1.00, Max: 4.00 | Avg Q: 2.59 | Eps: 0.47 | Loss: 0.0229\n",
      "Ep 11750: Reward 0.00, Min: -1.00, Max: 8.00 | Avg Q: 2.63 | Eps: 0.47 | Loss: 0.0390\n",
      "Ep 11800: Reward -1.00, Min: -1.00, Max: 9.00 | Avg Q: 2.60 | Eps: 0.47 | Loss: 0.0141\n",
      "Ep 11850: Reward 1.00, Min: -1.00, Max: 8.00 | Avg Q: 2.59 | Eps: 0.46 | Loss: 0.0482\n",
      "Ep 11900: Reward -1.00, Min: -1.00, Max: 4.00 | Avg Q: 2.61 | Eps: 0.46 | Loss: 0.0305\n",
      "Ep 11950: Reward -1.00, Min: -1.00, Max: 7.00 | Avg Q: 2.64 | Eps: 0.46 | Loss: 0.0518\n",
      "Ep 12000: Reward -1.00, Min: -1.00, Max: 5.00 | Avg Q: 2.62 | Eps: 0.46 | Loss: 0.0348\n",
      "Ep 12050: Reward 1.00, Min: -1.00, Max: 4.00 | Avg Q: 2.62 | Eps: 0.46 | Loss: 0.0359\n",
      "Ep 12100: Reward 1.00, Min: -1.00, Max: 5.00 | Avg Q: 2.61 | Eps: 0.46 | Loss: 0.0196\n",
      "Ep 12150: Reward 2.00, Min: -1.00, Max: 6.00 | Avg Q: 2.62 | Eps: 0.45 | Loss: 0.0126\n",
      "Ep 12200: Reward 0.00, Min: -1.00, Max: 5.00 | Avg Q: 2.65 | Eps: 0.45 | Loss: 0.0406\n",
      "Ep 12250: Reward -1.00, Min: -1.00, Max: 9.00 | Avg Q: 2.57 | Eps: 0.45 | Loss: 0.0488\n",
      "Ep 12300: Reward 1.00, Min: -1.00, Max: 5.00 | Avg Q: 2.59 | Eps: 0.45 | Loss: 0.0096\n",
      "Ep 12350: Reward -1.00, Min: -1.00, Max: 5.00 | Avg Q: 2.57 | Eps: 0.45 | Loss: 0.0272\n",
      "Ep 12400: Reward 0.00, Min: -1.00, Max: 7.00 | Avg Q: 2.61 | Eps: 0.45 | Loss: 0.0312\n",
      "Ep 12450: Reward 2.00, Min: -1.00, Max: 4.00 | Avg Q: 2.56 | Eps: 0.44 | Loss: 0.0094\n",
      "Ep 12500: Reward 0.00, Min: -1.00, Max: 3.00 | Avg Q: 2.60 | Eps: 0.44 | Loss: 0.0166\n",
      "Ep 12550: Reward 2.00, Min: -1.00, Max: 6.00 | Avg Q: 2.61 | Eps: 0.44 | Loss: 0.0237\n",
      "Ep 12600: Reward 0.00, Min: -1.00, Max: 5.00 | Avg Q: 2.62 | Eps: 0.44 | Loss: 0.0129\n",
      "Ep 12650: Reward 0.00, Min: -1.00, Max: 6.00 | Avg Q: 2.58 | Eps: 0.44 | Loss: 0.0281\n",
      "Ep 12700: Reward 7.00, Min: -1.00, Max: 7.00 | Avg Q: 2.59 | Eps: 0.44 | Loss: 0.0285\n",
      "Ep 12750: Reward 0.00, Min: -1.00, Max: 5.00 | Avg Q: 2.58 | Eps: 0.43 | Loss: 0.0262\n",
      "Ep 12800: Reward -1.00, Min: -1.00, Max: 5.00 | Avg Q: 2.58 | Eps: 0.43 | Loss: 0.0375\n",
      "Ep 12850: Reward 2.00, Min: -1.00, Max: 4.00 | Avg Q: 2.55 | Eps: 0.43 | Loss: 0.0511\n",
      "Ep 12900: Reward 2.00, Min: -1.00, Max: 5.00 | Avg Q: 2.58 | Eps: 0.43 | Loss: 0.0233\n",
      "Ep 12950: Reward 3.00, Min: -1.00, Max: 5.00 | Avg Q: 2.60 | Eps: 0.43 | Loss: 0.0275\n",
      "Ep 13000: Reward 0.00, Min: -1.00, Max: 11.00 | Avg Q: 2.60 | Eps: 0.42 | Loss: 0.0085\n",
      "Ep 13050: Reward 0.00, Min: -1.00, Max: 6.00 | Avg Q: 2.58 | Eps: 0.42 | Loss: 0.0124\n",
      "Ep 13100: Reward 6.00, Min: -1.00, Max: 6.00 | Avg Q: 2.57 | Eps: 0.42 | Loss: 0.0242\n",
      "Ep 13150: Reward 4.00, Min: -1.00, Max: 6.00 | Avg Q: 2.56 | Eps: 0.42 | Loss: 0.0099\n",
      "Ep 13200: Reward 1.00, Min: -1.00, Max: 7.00 | Avg Q: 2.53 | Eps: 0.42 | Loss: 0.0166\n",
      "Ep 13250: Reward 3.00, Min: -1.00, Max: 7.00 | Avg Q: 2.55 | Eps: 0.42 | Loss: 0.0377\n",
      "Ep 13300: Reward 0.00, Min: -1.00, Max: 9.00 | Avg Q: 2.57 | Eps: 0.41 | Loss: 0.0151\n",
      "Ep 13350: Reward 1.00, Min: -1.00, Max: 6.00 | Avg Q: 2.55 | Eps: 0.41 | Loss: 0.0283\n",
      "Ep 13400: Reward -1.00, Min: -1.00, Max: 6.00 | Avg Q: 2.59 | Eps: 0.41 | Loss: 0.0296\n",
      "Ep 13450: Reward 2.00, Min: -1.00, Max: 6.00 | Avg Q: 2.60 | Eps: 0.41 | Loss: 0.0171\n",
      "Ep 13500: Reward 1.00, Min: -1.00, Max: 6.00 | Avg Q: 2.60 | Eps: 0.41 | Loss: 0.0233\n",
      "Ep 13550: Reward 1.00, Min: -1.00, Max: 8.00 | Avg Q: 2.64 | Eps: 0.40 | Loss: 0.0261\n",
      "Ep 13600: Reward 0.00, Min: -1.00, Max: 8.00 | Avg Q: 2.68 | Eps: 0.40 | Loss: 0.0414\n",
      "Ep 13650: Reward 2.00, Min: -1.00, Max: 7.00 | Avg Q: 2.68 | Eps: 0.40 | Loss: 0.0275\n",
      "Ep 13700: Reward 2.00, Min: -1.00, Max: 6.00 | Avg Q: 2.68 | Eps: 0.40 | Loss: 0.0299\n",
      "Ep 13750: Reward 4.00, Min: -1.00, Max: 7.00 | Avg Q: 2.66 | Eps: 0.40 | Loss: 0.0081\n",
      "Ep 13800: Reward 11.00, Min: -1.00, Max: 11.00 | Avg Q: 2.69 | Eps: 0.39 | Loss: 0.0134\n",
      "Ep 13850: Reward -1.00, Min: -1.00, Max: 10.00 | Avg Q: 2.61 | Eps: 0.39 | Loss: 0.0228\n",
      "Ep 13900: Reward 1.00, Min: -1.00, Max: 10.00 | Avg Q: 2.65 | Eps: 0.39 | Loss: 0.0324\n",
      "Ep 13950: Reward 0.00, Min: -1.00, Max: 10.00 | Avg Q: 2.69 | Eps: 0.39 | Loss: 0.0124\n",
      "Ep 14000: Reward 3.00, Min: -1.00, Max: 7.00 | Avg Q: 2.64 | Eps: 0.39 | Loss: 0.0090\n",
      "Ep 14050: Reward 1.00, Min: -1.00, Max: 11.00 | Avg Q: 2.66 | Eps: 0.39 | Loss: 0.0094\n",
      "Ep 14100: Reward 2.00, Min: -1.00, Max: 7.00 | Avg Q: 2.62 | Eps: 0.38 | Loss: 0.0429\n",
      "Ep 14150: Reward 1.00, Min: -1.00, Max: 13.00 | Avg Q: 2.65 | Eps: 0.38 | Loss: 0.0291\n",
      "Ep 14200: Reward 0.00, Min: -1.00, Max: 10.00 | Avg Q: 2.65 | Eps: 0.38 | Loss: 0.0260\n",
      "Ep 14250: Reward 0.00, Min: -1.00, Max: 9.00 | Avg Q: 2.62 | Eps: 0.38 | Loss: 0.0426\n",
      "Ep 14300: Reward 0.00, Min: -1.00, Max: 9.00 | Avg Q: 2.64 | Eps: 0.38 | Loss: 0.0177\n",
      "Ep 14350: Reward -1.00, Min: -1.00, Max: 9.00 | Avg Q: 2.66 | Eps: 0.37 | Loss: 0.0339\n",
      "Ep 14400: Reward 1.00, Min: -1.00, Max: 8.00 | Avg Q: 2.70 | Eps: 0.37 | Loss: 0.0610\n",
      "Ep 14450: Reward 0.00, Min: -1.00, Max: 8.00 | Avg Q: 2.69 | Eps: 0.37 | Loss: 0.0116\n",
      "Ep 14500: Reward 1.00, Min: -1.00, Max: 6.00 | Avg Q: 2.69 | Eps: 0.37 | Loss: 0.0137\n",
      "Ep 14550: Reward 0.00, Min: -1.00, Max: 12.00 | Avg Q: 2.65 | Eps: 0.37 | Loss: 0.0256\n",
      "Ep 14600: Reward -1.00, Min: -1.00, Max: 10.00 | Avg Q: 2.65 | Eps: 0.37 | Loss: 0.0385\n",
      "Ep 14650: Reward 0.00, Min: -1.00, Max: 8.00 | Avg Q: 2.64 | Eps: 0.36 | Loss: 0.0250\n",
      "Ep 14700: Reward 0.00, Min: -1.00, Max: 7.00 | Avg Q: 2.65 | Eps: 0.36 | Loss: 0.0172\n",
      "Ep 14750: Reward 1.00, Min: -1.00, Max: 12.00 | Avg Q: 2.65 | Eps: 0.36 | Loss: 0.0160\n",
      "Ep 14800: Reward 0.00, Min: -1.00, Max: 9.00 | Avg Q: 2.71 | Eps: 0.36 | Loss: 0.0162\n",
      "Ep 14850: Reward 1.00, Min: -1.00, Max: 12.00 | Avg Q: 2.68 | Eps: 0.36 | Loss: 0.0128\n",
      "Ep 14900: Reward 2.00, Min: -1.00, Max: 7.00 | Avg Q: 2.66 | Eps: 0.36 | Loss: 0.0444\n",
      "Ep 14950: Reward 1.00, Min: -1.00, Max: 9.00 | Avg Q: 2.67 | Eps: 0.35 | Loss: 0.0234\n",
      "Ep 15000: Reward -1.00, Min: -1.00, Max: 7.00 | Avg Q: 2.66 | Eps: 0.35 | Loss: 0.0106\n",
      "Ep 15050: Reward 0.00, Min: -1.00, Max: 7.00 | Avg Q: 2.70 | Eps: 0.35 | Loss: 0.0305\n",
      "Ep 15100: Reward 1.00, Min: -1.00, Max: 8.00 | Avg Q: 2.70 | Eps: 0.35 | Loss: 0.0074\n",
      "Ep 15150: Reward 2.00, Min: -1.00, Max: 8.00 | Avg Q: 2.71 | Eps: 0.35 | Loss: 0.0247\n",
      "Ep 15200: Reward 1.00, Min: -1.00, Max: 11.00 | Avg Q: 2.71 | Eps: 0.35 | Loss: 0.0143\n",
      "Ep 15250: Reward 0.00, Min: -1.00, Max: 8.00 | Avg Q: 2.68 | Eps: 0.34 | Loss: 0.0112\n",
      "Ep 15300: Reward 0.00, Min: -1.00, Max: 8.00 | Avg Q: 2.70 | Eps: 0.34 | Loss: 0.0123\n",
      "Ep 15350: Reward 0.00, Min: -1.00, Max: 10.00 | Avg Q: 2.65 | Eps: 0.34 | Loss: 0.0281\n",
      "Ep 15400: Reward 0.00, Min: -1.00, Max: 14.00 | Avg Q: 2.69 | Eps: 0.34 | Loss: 0.0134\n",
      "Ep 15450: Reward 0.00, Min: -1.00, Max: 5.00 | Avg Q: 2.69 | Eps: 0.34 | Loss: 0.0209\n",
      "Ep 15500: Reward 0.00, Min: -1.00, Max: 9.00 | Avg Q: 2.68 | Eps: 0.34 | Loss: 0.0348\n",
      "Ep 15550: Reward 2.00, Min: -1.00, Max: 10.00 | Avg Q: 2.69 | Eps: 0.33 | Loss: 0.0502\n",
      "Ep 15600: Reward 2.00, Min: -1.00, Max: 10.00 | Avg Q: 2.66 | Eps: 0.33 | Loss: 0.0109\n",
      "Ep 15650: Reward 4.00, Min: -1.00, Max: 11.00 | Avg Q: 2.64 | Eps: 0.33 | Loss: 0.0331\n",
      "Ep 15700: Reward 4.00, Min: -1.00, Max: 7.00 | Avg Q: 2.65 | Eps: 0.33 | Loss: 0.0333\n",
      "Ep 15750: Reward 2.00, Min: -1.00, Max: 11.00 | Avg Q: 2.66 | Eps: 0.33 | Loss: 0.0243\n",
      "Ep 15800: Reward 0.00, Min: -1.00, Max: 12.00 | Avg Q: 2.64 | Eps: 0.33 | Loss: 0.0370\n",
      "Ep 15850: Reward 3.00, Min: -1.00, Max: 8.00 | Avg Q: 2.66 | Eps: 0.32 | Loss: 0.0263\n",
      "Ep 15900: Reward 1.00, Min: -1.00, Max: 16.00 | Avg Q: 2.65 | Eps: 0.32 | Loss: 0.0328\n",
      "Ep 15950: Reward 1.00, Min: -1.00, Max: 8.00 | Avg Q: 2.63 | Eps: 0.32 | Loss: 0.0169\n",
      "Ep 16000: Reward 3.00, Min: -1.00, Max: 9.00 | Avg Q: 2.61 | Eps: 0.32 | Loss: 0.0384\n",
      "Ep 16050: Reward 1.00, Min: -1.00, Max: 8.00 | Avg Q: 2.64 | Eps: 0.32 | Loss: 0.0348\n",
      "Ep 16100: Reward -1.00, Min: -1.00, Max: 11.00 | Avg Q: 2.64 | Eps: 0.32 | Loss: 0.0459\n",
      "Ep 16150: Reward 6.00, Min: -1.00, Max: 8.00 | Avg Q: 2.67 | Eps: 0.31 | Loss: 0.0213\n",
      "Ep 16200: Reward 1.00, Min: -1.00, Max: 15.00 | Avg Q: 2.60 | Eps: 0.31 | Loss: 0.0284\n",
      "Ep 16250: Reward 0.00, Min: -1.00, Max: 8.00 | Avg Q: 2.61 | Eps: 0.31 | Loss: 0.0225\n",
      "Ep 16300: Reward 2.00, Min: -1.00, Max: 8.00 | Avg Q: 2.67 | Eps: 0.31 | Loss: 0.0131\n",
      "Ep 16350: Reward 1.00, Min: -1.00, Max: 8.00 | Avg Q: 2.66 | Eps: 0.31 | Loss: 0.0142\n",
      "Ep 16400: Reward 0.00, Min: -1.00, Max: 9.00 | Avg Q: 2.67 | Eps: 0.31 | Loss: 0.0137\n",
      "Ep 16450: Reward 6.00, Min: -1.00, Max: 12.00 | Avg Q: 2.69 | Eps: 0.31 | Loss: 0.0234\n",
      "Ep 16500: Reward 0.00, Min: -1.00, Max: 10.00 | Avg Q: 2.68 | Eps: 0.30 | Loss: 0.0069\n",
      "Ep 16550: Reward 2.00, Min: -1.00, Max: 10.00 | Avg Q: 2.69 | Eps: 0.30 | Loss: 0.0242\n",
      "Ep 16600: Reward 3.00, Min: -1.00, Max: 12.00 | Avg Q: 2.69 | Eps: 0.30 | Loss: 0.0142\n",
      "Ep 16650: Reward 2.00, Min: -1.00, Max: 10.00 | Avg Q: 2.69 | Eps: 0.30 | Loss: 0.0344\n",
      "Ep 16700: Reward 2.00, Min: -1.00, Max: 8.00 | Avg Q: 2.63 | Eps: 0.30 | Loss: 0.0150\n",
      "Ep 16750: Reward 4.00, Min: -1.00, Max: 11.00 | Avg Q: 2.66 | Eps: 0.29 | Loss: 0.0309\n",
      "Ep 16800: Reward 7.00, Min: -1.00, Max: 10.00 | Avg Q: 2.69 | Eps: 0.29 | Loss: 0.0225\n",
      "Ep 16850: Reward 2.00, Min: -1.00, Max: 9.00 | Avg Q: 2.70 | Eps: 0.29 | Loss: 0.0392\n",
      "Ep 16900: Reward 4.00, Min: -1.00, Max: 9.00 | Avg Q: 2.70 | Eps: 0.29 | Loss: 0.0101\n",
      "Ep 16950: Reward 3.00, Min: -1.00, Max: 8.00 | Avg Q: 2.68 | Eps: 0.29 | Loss: 0.0277\n",
      "Ep 17000: Reward 0.00, Min: -1.00, Max: 11.00 | Avg Q: 2.67 | Eps: 0.29 | Loss: 0.0313\n",
      "Ep 17050: Reward 0.00, Min: -1.00, Max: 12.00 | Avg Q: 2.71 | Eps: 0.29 | Loss: 0.0532\n",
      "Ep 17100: Reward 9.00, Min: -1.00, Max: 12.00 | Avg Q: 2.68 | Eps: 0.28 | Loss: 0.0422\n",
      "Ep 17150: Reward 1.00, Min: -1.00, Max: 9.00 | Avg Q: 2.62 | Eps: 0.28 | Loss: 0.0262\n",
      "Ep 17200: Reward 0.00, Min: -1.00, Max: 9.00 | Avg Q: 2.64 | Eps: 0.28 | Loss: 0.0123\n",
      "Ep 17250: Reward 0.00, Min: -1.00, Max: 9.00 | Avg Q: 2.71 | Eps: 0.28 | Loss: 0.0276\n",
      "Ep 17300: Reward 3.00, Min: -1.00, Max: 12.00 | Avg Q: 2.73 | Eps: 0.28 | Loss: 0.0212\n",
      "Ep 17350: Reward 2.00, Min: -1.00, Max: 12.00 | Avg Q: 2.73 | Eps: 0.28 | Loss: 0.0273\n",
      "Ep 17400: Reward 0.00, Min: -1.00, Max: 11.00 | Avg Q: 2.73 | Eps: 0.27 | Loss: 0.0185\n",
      "Ep 17450: Reward 0.00, Min: -1.00, Max: 13.00 | Avg Q: 2.73 | Eps: 0.27 | Loss: 0.0132\n",
      "Ep 17500: Reward 7.00, Min: -1.00, Max: 10.00 | Avg Q: 2.69 | Eps: 0.27 | Loss: 0.0299\n",
      "Ep 17550: Reward -1.00, Min: -1.00, Max: 11.00 | Avg Q: 2.69 | Eps: 0.27 | Loss: 0.0382\n",
      "Ep 17600: Reward 0.00, Min: -1.00, Max: 10.00 | Avg Q: 2.72 | Eps: 0.27 | Loss: 0.0112\n",
      "Ep 17650: Reward 1.00, Min: -1.00, Max: 17.00 | Avg Q: 2.72 | Eps: 0.27 | Loss: 0.0177\n",
      "Ep 17700: Reward 4.00, Min: -1.00, Max: 9.00 | Avg Q: 2.74 | Eps: 0.26 | Loss: 0.0211\n",
      "Ep 17750: Reward 3.00, Min: -1.00, Max: 11.00 | Avg Q: 2.70 | Eps: 0.26 | Loss: 0.0083\n",
      "Ep 17800: Reward 4.00, Min: -1.00, Max: 14.00 | Avg Q: 2.71 | Eps: 0.26 | Loss: 0.0222\n",
      "Ep 17850: Reward 10.00, Min: -1.00, Max: 10.00 | Avg Q: 2.71 | Eps: 0.26 | Loss: 0.0207\n",
      "Ep 17900: Reward 5.00, Min: -1.00, Max: 14.00 | Avg Q: 2.69 | Eps: 0.26 | Loss: 0.0231\n",
      "Ep 17950: Reward 5.00, Min: -1.00, Max: 15.00 | Avg Q: 2.70 | Eps: 0.26 | Loss: 0.0137\n",
      "Ep 18000: Reward -1.00, Min: -1.00, Max: 10.00 | Avg Q: 2.67 | Eps: 0.25 | Loss: 0.0280\n",
      "Ep 18050: Reward 3.00, Min: -1.00, Max: 13.00 | Avg Q: 2.66 | Eps: 0.25 | Loss: 0.0206\n",
      "Ep 18100: Reward 2.00, Min: -1.00, Max: 13.00 | Avg Q: 2.67 | Eps: 0.25 | Loss: 0.0175\n",
      "Ep 18150: Reward 1.00, Min: -1.00, Max: 12.00 | Avg Q: 2.69 | Eps: 0.25 | Loss: 0.0137\n",
      "Ep 18200: Reward 1.00, Min: -1.00, Max: 12.00 | Avg Q: 2.66 | Eps: 0.25 | Loss: 0.0105\n",
      "Ep 18250: Reward 0.00, Min: -1.00, Max: 9.00 | Avg Q: 2.68 | Eps: 0.25 | Loss: 0.0141\n",
      "Ep 18300: Reward 3.00, Min: 0.00, Max: 12.00 | Avg Q: 2.72 | Eps: 0.24 | Loss: 0.0120\n",
      "Ep 18350: Reward 2.00, Min: -1.00, Max: 17.00 | Avg Q: 2.67 | Eps: 0.24 | Loss: 0.0113\n",
      "Ep 18400: Reward 0.00, Min: -1.00, Max: 11.00 | Avg Q: 2.71 | Eps: 0.24 | Loss: 0.0187\n",
      "Ep 18450: Reward 9.00, Min: 0.00, Max: 11.00 | Avg Q: 2.72 | Eps: 0.24 | Loss: 0.0171\n",
      "Ep 18500: Reward 0.00, Min: -1.00, Max: 12.00 | Avg Q: 2.70 | Eps: 0.24 | Loss: 0.0132\n",
      "Ep 18550: Reward 4.00, Min: -1.00, Max: 16.00 | Avg Q: 2.72 | Eps: 0.24 | Loss: 0.0215\n",
      "Ep 18600: Reward 0.00, Min: -1.00, Max: 12.00 | Avg Q: 2.66 | Eps: 0.23 | Loss: 0.0261\n",
      "Ep 18650: Reward 12.00, Min: -1.00, Max: 12.00 | Avg Q: 2.70 | Eps: 0.23 | Loss: 0.0138\n",
      "Ep 18700: Reward -1.00, Min: -1.00, Max: 15.00 | Avg Q: 2.68 | Eps: 0.23 | Loss: 0.0250\n",
      "Ep 18750: Reward 0.00, Min: -1.00, Max: 12.00 | Avg Q: 2.71 | Eps: 0.23 | Loss: 0.0299\n",
      "Ep 18800: Reward 2.00, Min: -1.00, Max: 15.00 | Avg Q: 2.66 | Eps: 0.23 | Loss: 0.0221\n",
      "Ep 18850: Reward 0.00, Min: -1.00, Max: 14.00 | Avg Q: 2.70 | Eps: 0.23 | Loss: 0.0190\n",
      "Ep 18900: Reward 2.00, Min: -1.00, Max: 12.00 | Avg Q: 2.68 | Eps: 0.23 | Loss: 0.0420\n",
      "Ep 18950: Reward 6.00, Min: -1.00, Max: 11.00 | Avg Q: 2.71 | Eps: 0.22 | Loss: 0.0266\n",
      "Ep 19000: Reward 10.00, Min: 0.00, Max: 16.00 | Avg Q: 2.72 | Eps: 0.22 | Loss: 0.0266\n",
      "Ep 19050: Reward 0.00, Min: 0.00, Max: 16.00 | Avg Q: 2.67 | Eps: 0.22 | Loss: 0.0279\n",
      "Ep 19100: Reward 11.00, Min: -1.00, Max: 19.00 | Avg Q: 2.69 | Eps: 0.22 | Loss: 0.0227\n",
      "Ep 19150: Reward 4.00, Min: -1.00, Max: 12.00 | Avg Q: 2.73 | Eps: 0.22 | Loss: 0.0245\n",
      "Ep 19200: Reward 2.00, Min: -1.00, Max: 19.00 | Avg Q: 2.70 | Eps: 0.22 | Loss: 0.0366\n",
      "Ep 19250: Reward 0.00, Min: -1.00, Max: 14.00 | Avg Q: 2.76 | Eps: 0.21 | Loss: 0.0165\n",
      "Ep 19300: Reward 4.00, Min: -1.00, Max: 13.00 | Avg Q: 2.71 | Eps: 0.21 | Loss: 0.0180\n",
      "Ep 19350: Reward 4.00, Min: -1.00, Max: 13.00 | Avg Q: 2.72 | Eps: 0.21 | Loss: 0.0202\n",
      "Ep 19400: Reward 6.00, Min: -1.00, Max: 12.00 | Avg Q: 2.68 | Eps: 0.21 | Loss: 0.0193\n",
      "Ep 19450: Reward 10.00, Min: -1.00, Max: 13.00 | Avg Q: 2.69 | Eps: 0.21 | Loss: 0.0194\n",
      "Ep 19500: Reward 8.00, Min: -1.00, Max: 15.00 | Avg Q: 2.71 | Eps: 0.21 | Loss: 0.0114\n",
      "Ep 19550: Reward 0.00, Min: -1.00, Max: 15.00 | Avg Q: 2.68 | Eps: 0.21 | Loss: 0.0246\n",
      "Ep 19600: Reward 0.00, Min: -1.00, Max: 11.00 | Avg Q: 2.63 | Eps: 0.21 | Loss: 0.0352\n",
      "Ep 19650: Reward 0.00, Min: -1.00, Max: 17.00 | Avg Q: 2.66 | Eps: 0.20 | Loss: 0.0458\n",
      "Ep 19700: Reward 2.00, Min: -1.00, Max: 19.00 | Avg Q: 2.71 | Eps: 0.20 | Loss: 0.0264\n",
      "Ep 19750: Reward 0.00, Min: -1.00, Max: 12.00 | Avg Q: 2.72 | Eps: 0.20 | Loss: 0.0183\n",
      "Ep 19800: Reward 0.00, Min: -1.00, Max: 13.00 | Avg Q: 2.71 | Eps: 0.20 | Loss: 0.0249\n",
      "Ep 19850: Reward 2.00, Min: -1.00, Max: 17.00 | Avg Q: 2.73 | Eps: 0.20 | Loss: 0.0181\n",
      "Ep 19900: Reward 0.00, Min: -1.00, Max: 18.00 | Avg Q: 2.70 | Eps: 0.20 | Loss: 0.0431\n",
      "Ep 19950: Reward 0.00, Min: -1.00, Max: 22.00 | Avg Q: 2.67 | Eps: 0.19 | Loss: 0.0574\n"
     ]
    }
   ],
   "source": [
    "results = {}\n",
    "N_EPISODES = 20000\n",
    "\n",
    "# 1. Train Standard DQN\n",
    "# print(\"Collecting data for Standard DQN...\")\n",
    "# res_dqn = run_snake_game_experiment('DQN', double_dqn=False, total_episodes=N_EPISODES)\n",
    "# results['Standard DQN'] = res_dqn\n",
    "\n",
    "# 2. Train Double DQN\n",
    "print(\"\\nCollecting data for Double DQN...\")\n",
    "res_ddqn = run_snake_game_experiment('Double DQN', double_dqn=True, total_episodes=N_EPISODES)\n",
    "results['Double DQN'] = res_ddqn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df9b9967-4d23-41d9-816b-f06934ccb517",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (rl_py311_new)",
   "language": "python",
   "name": "rl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
