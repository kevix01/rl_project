{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fdd56ed6-9d0a-4345-86c0-f188a14f03c1",
   "metadata": {},
   "source": [
    "Iperparametri corrispondenti ai file con i pesi della rete, ottenuti dall'esecuzione di vari esperimenti \n",
    "- trained_model.pth --> rete 32-16, epsilon_decay=300000, gamma=0.95, buffer_capacity=100000\n",
    "- trained_model1.pth --> rete 64-32, epsilon_decay=150000, gamma=0.95, buffer_capacity=100000 (best up to now)\n",
    "- trained_model2.pth --> rete 64-32, epsilon_decay=175000, gamma=0.95, buffer_capacity=100000\n",
    "- trained_model3.pth --> rete 64-32, epsilon_decay=150000, gamma=0.99, buffer_capacity=100000 (looping) XXXXXX\n",
    "- trained_model4.pth --> rete 64-32, epsilon_decay=150000, gamma=0.95, buffer_capacity=20000 (looping) XXXXXXX\n",
    "- Observation space cambiato: [head_x, head_y, food_dx, food_dy, dist_up, dist_down, dist_left, dist_right, surr_up, surr_down, surr_left, surr_right, tail_dx, tail_dy, dir_x, dir_y] --> 16 componenti invece che 8\n",
    "    - https://gemini.google.com/share/a3c736821d16\n",
    "    - trained_model5.pth --> rete 64-32, epsilon_decay=150000, gamma=0.95, buffer_capacity=20000, lr=1e-4, target_update_freq=1000, batch_size=64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "efdc2b33-227a-4798-92ff-677b81219d61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "import gym_snakegame\n",
    "import gymnasium as gym\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from collections import deque\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f74460bd-5829-4a46-9cc2-57e73ec513ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        super(DQN, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(state_dim, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, action_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "def select_action(model, state, epsilon, act_dim):\n",
    "    # Exploration: random action\n",
    "    if random.random() < epsilon:\n",
    "        return random.randrange(act_dim)\n",
    "    # Exploitation: best action according to the network\n",
    "    with torch.no_grad():\n",
    "        # Inference: NN estimates action-value function\n",
    "        state = torch.tensor(state, dtype=torch.float32).unsqueeze(0).to(DEVICE)\n",
    "        return int(torch.argmax(model(state)).item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56e8dc3f-c966-4d70-a9d2-cf53943b79bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\utente\\anaconda3\\envs\\rl-env\\Lib\\site-packages\\pygame\\pkgdata.py:25: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  from pkg_resources import resource_stream, resource_exists\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modello caricato da: trained_model5.pth\n",
      "Avvio di 50 partite di test...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\utente\\anaconda3\\envs\\rl-env\\Lib\\site-packages\\gymnasium\\utils\\passive_env_checker.py:188: UserWarning: \u001b[33mWARN: The obs returned by the `reset()` method is not within the observation space.\u001b[0m\n",
      "  logger.warn(f\"{pre} is not within the observation space.\")\n",
      "C:\\Users\\utente\\anaconda3\\envs\\rl-env\\Lib\\site-packages\\gymnasium\\utils\\passive_env_checker.py:188: UserWarning: \u001b[33mWARN: The obs returned by the `step()` method is not within the observation space.\u001b[0m\n",
      "  logger.warn(f\"{pre} is not within the observation space.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Inizio Partita 1/50 ---\n",
      "Partita 1 terminata! Reward totale: 19\n",
      "\n",
      "--- Inizio Partita 2/50 ---\n",
      "Partita 2 terminata! Reward totale: 19\n",
      "\n",
      "--- Inizio Partita 3/50 ---\n",
      "Partita 3 terminata! Reward totale: 18\n",
      "\n",
      "--- Inizio Partita 4/50 ---\n",
      "Partita 4 terminata! Reward totale: 18\n",
      "\n",
      "--- Inizio Partita 5/50 ---\n",
      "Partita 5 terminata! Reward totale: 28\n",
      "\n",
      "--- Inizio Partita 6/50 ---\n",
      "Partita 6 terminata! Reward totale: 6\n",
      "\n",
      "--- Inizio Partita 7/50 ---\n",
      "Partita 7 terminata! Reward totale: 15\n",
      "\n",
      "--- Inizio Partita 8/50 ---\n",
      "Partita 8 terminata! Reward totale: 20\n",
      "\n",
      "--- Inizio Partita 9/50 ---\n"
     ]
    }
   ],
   "source": [
    "def watch_agent_play(model_path, board_size=10, num_episodes=50):\n",
    "    \"\"\"\n",
    "    Carica un modello e visualizza un numero specificato di partite.\n",
    "    \n",
    "    Args:\n",
    "        model_path (str): Percorso del file .pth\n",
    "        board_size (int): Dimensione della mappa\n",
    "        num_episodes (int): Numero di partite da giocare\n",
    "    \"\"\"\n",
    "    \n",
    "    # 1. Configura l'ambiente\n",
    "    env = gym.make(\n",
    "        \"gym_snakegame/SnakeGame-v0\",\n",
    "        board_size=board_size,\n",
    "        n_channel=1,\n",
    "        n_target=1,\n",
    "        render_mode='human'\n",
    "    )\n",
    "\n",
    "    # 2. Ricalcola le dimensioni\n",
    "    obs_dim = np.prod(env.observation_space.shape)\n",
    "    action_dim = env.action_space.n\n",
    "\n",
    "    # 3. Inizializza la rete e carica i pesi\n",
    "    model = DQN(obs_dim, action_dim).to(DEVICE)\n",
    "    \n",
    "    try:\n",
    "        model.load_state_dict(torch.load(model_path, map_location=DEVICE))\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Errore: Il file '{model_path}' non è stato trovato.\")\n",
    "        return\n",
    "\n",
    "    model.eval()\n",
    "    print(f\"Modello caricato da: {model_path}\")\n",
    "    print(f\"Avvio di {num_episodes} partite di test...\")\n",
    "\n",
    "    all_rewards = []\n",
    "\n",
    "    # --- CICLO DEGLI EPISODI ---\n",
    "    for i_ep in range(num_episodes):\n",
    "        state, _ = env.reset()\n",
    "        done = False\n",
    "        total_reward = 0\n",
    "        \n",
    "        print(f\"\\n--- Inizio Partita {i_ep + 1}/{num_episodes} ---\")\n",
    "\n",
    "        while not done:\n",
    "            # Prepara lo stato\n",
    "            state_tensor = torch.FloatTensor(state).unsqueeze(0).to(DEVICE)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                q_values = model(state_tensor)\n",
    "                action = q_values.argmax(dim=1).item()\n",
    "\n",
    "            state, reward, terminated, truncated, _ = env.step(action)\n",
    "            done = terminated or truncated\n",
    "            total_reward += reward\n",
    "            \n",
    "            # Opzionale: rallenta leggermente per rendere l'azione più visibile all'occhio umano\n",
    "            # time.sleep(0.05) \n",
    "\n",
    "        # Fine dell'episodio corrente\n",
    "        print(f\"Partita {i_ep + 1} terminata! Reward totale: {total_reward}\")\n",
    "        all_rewards.append(total_reward)\n",
    "        \n",
    "        # Pausa breve tra una partita e l'altra\n",
    "        time.sleep(1.0)\n",
    "\n",
    "    env.close()\n",
    "    \n",
    "    # Statistiche finali\n",
    "    mean_reward = np.mean(all_rewards)\n",
    "    print(\"\\n==================================\")\n",
    "    print(f\"Sessione conclusa.\")\n",
    "    print(f\"Partite giocate: {num_episodes}\")\n",
    "    print(f\"Reward Medio: {mean_reward:.2f}\")\n",
    "    print(f\"Miglior Partita: {np.max(all_rewards)}\")\n",
    "    print(\"==================================\")\n",
    "\n",
    "# --- ESECUZIONE ---\n",
    "# Esempio: Visualizza 3 partite\n",
    "watch_agent_play(\"trained_model5.pth\", num_episodes=50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RL Environment",
   "language": "python",
   "name": "rl-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
